{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!tar xvzf aclImdb_small.tgz > /dev/null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/Sandy/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "nltk.download('punkt')\n",
    "import torch\n",
    "\n",
    "#Sparse matrix implementation\n",
    "from scipy.sparse import csr_matrix\n",
    "import scipy.sparse as sp\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "\n",
    "np.random.seed(1)\n",
    "\n",
    "class Vocab:\n",
    "    def __init__(self, vocabFile=None):\n",
    "        self.locked = False\n",
    "        self.nextId = 0\n",
    "        self.word2id = {}\n",
    "        self.id2word = {}\n",
    "        if vocabFile:\n",
    "            for line in open(vocabFile):\n",
    "                line = line.rstrip('\\n')\n",
    "                (word, wid) = line.split('\\t')\n",
    "                self.word2id[word] = int(wid)\n",
    "                self.id2word[wid] = word\n",
    "                self.nextId = max(self.nextId, int(wid) + 1)\n",
    "\n",
    "    def GetID(self, word):\n",
    "        if not word in self.word2id:\n",
    "            if self.locked:\n",
    "                return -1        #UNK token is -1.\n",
    "            else:\n",
    "                self.word2id[word] = self.nextId\n",
    "                self.id2word[self.word2id[word]] = word\n",
    "                self.nextId += 1\n",
    "        return self.word2id[word]\n",
    "\n",
    "    def HasWord(self, word):\n",
    "        return self.word2id.has_key(word)\n",
    "\n",
    "    def HasId(self, wid):\n",
    "        return self.id2word.has_key(wid)\n",
    "\n",
    "    def GetWord(self, wid):\n",
    "        return self.id2word[wid]\n",
    "\n",
    "    def SaveVocab(self, vocabFile):\n",
    "        fOut = open(vocabFile, 'w')\n",
    "        for word in self.word2id.keys():\n",
    "            fOut.write(\"%s\\t%s\\n\" % (word, self.word2id[word]))\n",
    "\n",
    "    def GetVocabSize(self):\n",
    "        #return self.nextId-1\n",
    "        return self.nextId\n",
    "\n",
    "    def GetWords(self):\n",
    "        return self.word2id.keys()\n",
    "\n",
    "    def Lock(self):\n",
    "        self.locked = True\n",
    "\n",
    "class IMDBdata:\n",
    "    def __init__(self, directory, vocab=None):\n",
    "        \"\"\" Reads in data into sparse matrix format \"\"\"\n",
    "        pFiles = os.listdir(\"%s/pos\" % directory)\n",
    "        nFiles = os.listdir(\"%s/neg\" % directory)\n",
    "\n",
    "        if not vocab:\n",
    "            self.vocab = Vocab()\n",
    "        else:\n",
    "            self.vocab = vocab\n",
    "\n",
    "        #For csr_matrix (see http://docs.scipy.org/doc/scipy-0.15.1/reference/generated/scipy.sparse.csr_matrix.html#scipy.sparse.csr_matrix)\n",
    "        X_values = []\n",
    "        X_row_indices = []\n",
    "        X_col_indices = []\n",
    "        Y = []\n",
    "\n",
    "        XwordList = []\n",
    "        XfileList = []\n",
    "\n",
    "        #Read positive files\n",
    "        for i in range(len(pFiles)):\n",
    "            f = pFiles[i]\n",
    "            for line in open(\"%s/pos/%s\" % (directory, f)):\n",
    "                wordList   = [self.vocab.GetID(w.lower()) for w in word_tokenize(line) if self.vocab.GetID(w.lower()) >= 0]\n",
    "                XwordList.append(wordList)\n",
    "                XfileList.append(f)\n",
    "                wordCounts = Counter(wordList)\n",
    "                for (wordId, count) in wordCounts.items():\n",
    "                    if wordId >= 0:\n",
    "                        X_row_indices.append(i)\n",
    "                        X_col_indices.append(wordId)\n",
    "                        X_values.append(count)\n",
    "            Y.append(+1.0)\n",
    "\n",
    "        #Read negative files\n",
    "        for i in range(len(nFiles)):\n",
    "            f = nFiles[i]\n",
    "            for line in open(\"%s/neg/%s\" % (directory, f)):\n",
    "                wordList   = [self.vocab.GetID(w.lower()) for w in word_tokenize(line) if self.vocab.GetID(w.lower()) >= 0]\n",
    "                XwordList.append(wordList)\n",
    "                XfileList.append(f)\n",
    "                wordCounts = Counter(wordList)\n",
    "                for (wordId, count) in wordCounts.items():\n",
    "                    if wordId >= 0:\n",
    "                        X_row_indices.append(len(pFiles)+i)\n",
    "                        X_col_indices.append(wordId)\n",
    "                        X_values.append(count)\n",
    "            Y.append(-1.0)\n",
    "            \n",
    "        self.vocab.Lock()\n",
    "\n",
    "        #Create a sparse matrix in csr format\n",
    "        self.X = csr_matrix((X_values, (X_row_indices, X_col_indices)), shape=(max(X_row_indices)+1, self.vocab.GetVocabSize()))\n",
    "        self.Y = np.asarray(Y)\n",
    "\n",
    "        #Randomly shuffle\n",
    "        index = np.arange(self.X.shape[0])\n",
    "        np.random.shuffle(index)\n",
    "        self.X = self.X[index,:]\n",
    "        self.XwordList = [torch.LongTensor(XwordList[i]) for i in index]  #Two different sparse formats, csr and lists of IDs (XwordList).\n",
    "        self.XfileList = [XfileList[i] for i in index]\n",
    "        self.Y = self.Y[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'self' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-29-660a45395a84>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'self' is not defined"
     ]
    }
   ],
   "source": [
    "self.X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = IMDBdata(\"aclImdb_small/train\")\n",
    "train.vocab.Lock()\n",
    "dev = IMDBdata(\"aclImdb_small/dev\", vocab=train.vocab)\n",
    "test = IMDBdata(\"aclImdb_small/test\", vocab=train.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7222, 57205)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7222,)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.Y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 4)\t5\n",
      "  (0, 9)\t2\n",
      "  (0, 12)\t8\n",
      "  (0, 14)\t16\n",
      "  (0, 16)\t12\n",
      "  (0, 17)\t1\n",
      "  (0, 20)\t3\n",
      "  (0, 21)\t8\n",
      "  (0, 22)\t2\n",
      "  (0, 23)\t15\n",
      "  (0, 30)\t5\n",
      "  (0, 31)\t12\n",
      "  (0, 33)\t15\n",
      "  (0, 35)\t3\n",
      "  (0, 36)\t7\n",
      "  (0, 40)\t32\n",
      "  (0, 43)\t1\n",
      "  (0, 46)\t4\n",
      "  (0, 48)\t1\n",
      "  (0, 50)\t14\n",
      "  (0, 60)\t1\n",
      "  (0, 68)\t4\n",
      "  (0, 72)\t1\n",
      "  (0, 75)\t18\n",
      "  (0, 82)\t7\n",
      "  :\t:\n",
      "  (0, 40422)\t1\n",
      "  (0, 40423)\t1\n",
      "  (0, 40424)\t1\n",
      "  (0, 40425)\t1\n",
      "  (0, 40426)\t1\n",
      "  (0, 40427)\t1\n",
      "  (0, 40428)\t1\n",
      "  (0, 40429)\t2\n",
      "  (0, 40430)\t1\n",
      "  (0, 40431)\t1\n",
      "  (0, 40432)\t1\n",
      "  (0, 40433)\t1\n",
      "  (0, 40434)\t2\n",
      "  (0, 40435)\t1\n",
      "  (0, 40436)\t1\n",
      "  (0, 40437)\t1\n",
      "  (0, 40438)\t1\n",
      "  (0, 40439)\t1\n",
      "  (0, 40440)\t1\n",
      "  (0, 40441)\t1\n",
      "  (0, 40442)\t1\n",
      "  (0, 40443)\t1\n",
      "  (0, 40444)\t1\n",
      "  (0, 40445)\t1\n",
      "  (0, 40446)\t1\n"
     ]
    }
   ],
   "source": [
    "train.X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
